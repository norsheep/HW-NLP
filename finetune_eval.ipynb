{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装opencompass：Kaggle上已经为我们准备好了其他常用包，只需安装opencompass用于评测即可。如果不在Kaggle上运行，则还需要安装其他必要包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install \"opencompass[full]\"\n",
    "# !pip install pytorch transformers datasets \"opencompass[full]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指令微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:42:50.738461Z",
     "iopub.status.busy": "2024-12-06T12:42:50.738204Z",
     "iopub.status.idle": "2024-12-06T12:42:50.743023Z",
     "shell.execute_reply": "2024-12-06T12:42:50.742025Z",
     "shell.execute_reply.started": "2024-12-06T12:42:50.738434Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yysong/miniconda3/envs/yyhw/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The main program for finetuning LLMs with Huggingface Transformers Library.\n",
    "\n",
    "ALL SECTIONS WHERE CODE POSSIBLY NEEDS TO BE FILLED IN ARE MARKED AS TODO.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict\n",
    "from transformers import TrainingArguments, HfArgumentParser, Trainer, AutoTokenizer, AutoModelForCausalLM\n",
    "import sys\n",
    "import torch\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:42:54.214990Z",
     "iopub.status.busy": "2024-12-06T12:42:54.214309Z",
     "iopub.status.idle": "2024-12-06T12:42:54.220805Z",
     "shell.execute_reply": "2024-12-06T12:42:54.219860Z",
     "shell.execute_reply.started": "2024-12-06T12:42:54.214952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the arguments required for the main program.\n",
    "# NOTE: You can customize any arguments you need to pass in.\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"Arguments for model\n",
    "    \"\"\"\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the LLM to fine-tune or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype.\"\n",
    "            ),\n",
    "            \"choices\": [\"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    # TODO: add your model arguments here\n",
    "    model_name_or_path = \"Qwen2.5-0.5B\"\n",
    "    torch_dtype = \"float32\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"Arguments for data\n",
    "    \"\"\"\n",
    "    dataset_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the fine-tuning dataset or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    # TODO: add your data arguments here\n",
    "    dataset_path = \"alpaca-cleaned/alpaca_data_cleaned.json\"\n",
    "    # 该数据集格式为 list[dict],每个dict包含instruction,input,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('json', data_files=\"alpaca-cleaned/alpaca_data_cleaned.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('train', Dataset({\n",
      "    features: ['output', 'instruction', 'input'],\n",
      "    num_rows: 51760\n",
      "}))])\n",
      "{'output': ['1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.', 'The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).', \"An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\\n\\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\\n\\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \\n\\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\", 'There are several ways to reduce air pollution, including:\\n\\n1. Reduce energy consumption: By conserving energy, we reduce the amount of pollution emitted from power plants. Some ways to achieve this include: using energy-efficient appliances and lighting, insulating homes, and shutting off lights and electronics when not in use.\\n\\n2. Use public transportation or carpool: Cars and trucks are major sources of pollution. By using public transportation, carpooling or biking, you can reduce the number of cars on the road, which in turn reduces emissions.\\n\\n3. Be responsible with waste: When waste is burned or decomposed, they can release harmful gases into the air. Dispose of hazardous waste correctly and recycle materials like plastics, paper, and glass when possible.\\n\\n4. Support clean energy sources: Renewable energy sources like solar and wind power generate little or no pollution. Support clean energy companies and projects, and consider installing renewable energy systems in your home.\\n\\n5. Drive efficiently: If you need to drive, make sure your vehicle is well maintained and drive efficiently by avoiding rapid acceleration and braking, and reducing idling.\\n\\n6. Be mindful of product choices: Some products and practices release more pollution than others. Choose products with eco-friendly packaging and try to minimize your use of aerosol sprays, which can release harmful chemicals into the air.', 'I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.'], 'instruction': ['Give three tips for staying healthy.', 'What are the three primary colors?', 'Describe the structure of an atom.', 'How can we reduce air pollution?', 'Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.'], 'input': ['', '', '', '', '']}\n",
      "['Give three tips for staying healthy.', 'What are the three primary colors?', 'Describe the structure of an atom.', 'How can we reduce air pollution?', 'Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.']\n",
      "['1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.', 'The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).', \"An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\\n\\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\\n\\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \\n\\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\", 'There are several ways to reduce air pollution, including:\\n\\n1. Reduce energy consumption: By conserving energy, we reduce the amount of pollution emitted from power plants. Some ways to achieve this include: using energy-efficient appliances and lighting, insulating homes, and shutting off lights and electronics when not in use.\\n\\n2. Use public transportation or carpool: Cars and trucks are major sources of pollution. By using public transportation, carpooling or biking, you can reduce the number of cars on the road, which in turn reduces emissions.\\n\\n3. Be responsible with waste: When waste is burned or decomposed, they can release harmful gases into the air. Dispose of hazardous waste correctly and recycle materials like plastics, paper, and glass when possible.\\n\\n4. Support clean energy sources: Renewable energy sources like solar and wind power generate little or no pollution. Support clean energy companies and projects, and consider installing renewable energy systems in your home.\\n\\n5. Drive efficiently: If you need to drive, make sure your vehicle is well maintained and drive efficiently by avoiding rapid acceleration and braking, and reducing idling.\\n\\n6. Be mindful of product choices: Some products and practices release more pollution than others. Choose products with eco-friendly packaging and try to minimize your use of aerosol sprays, which can release harmful chemicals into the air.', 'I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.items())\n",
    "print(dataset['train'][:5])\n",
    "\n",
    "model_path = \"./Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "input_text = [sample[\"instruction\"] + sample[\"input\"] for sample in dataset['train']] \n",
    "label_text = [sample[\"output\"] for sample in dataset['train']]\n",
    "print(input_text[:5])\n",
    "print(label_text[:5])\n",
    "input_ids = tokenizer(input_text, padding=True, truncation=True, max_length=2048,return_tensors=\"pt\")\n",
    "output_ids = tokenizer(label_text, padding=True, truncation=True, max_length=2048, return_tensors=\"pt\")\n",
    "input_ids['label'] = output_ids.input_ids\n",
    "\n",
    "# 'input_ids' and 'attention_mask' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = datasets.load_dataset('json', data_files=\"alpaca-cleaned/alpaca_data_cleaned.json\")\n",
    "def data_collator(batch: List[Dict]):\n",
    "        \"\"\"\n",
    "        batch: list of dict, each dict of the list is a sample in the dataset.\n",
    "        \"\"\"\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        max_length = 0\n",
    "\n",
    "        model_path = \"./Qwen2.5-0.5B\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        for sample in batch:\n",
    "            instruction = sample.get(\"instruction\",\"\")\n",
    "            input_text = sample.get(\"input\",\"\")\n",
    "            output_text = sample.get(\"output\",\"\")\n",
    "\n",
    "            # 检查input和output是否为空\n",
    "            # if not input_text.strip():\n",
    "            #     input_text = f\"\\n{input_text}\"\n",
    "\n",
    "            if not output_text.strip():\n",
    "                output_text = \"<empty>\"\n",
    "            \n",
    "            # 构建输入序列\n",
    "            input_ids = tokenizer.encode_plus(\n",
    "                f\"{instruction}{input_text}\", # 将instruction和input_text进行拼接，生成文本输入\n",
    "                return_tensors = \"pt\", # 输出转换为pytorch的张量格式\n",
    "                max_length = tokenizer.model_max_length, # 如果输入序列超过最大长度则截断\n",
    "                truncation = True, \n",
    "                padding = False, # 即使输入序列没有达到最大长度，也不进行填充\n",
    "            ).input_ids # 用于获取tokenizer返回字典中的‘input_ids’字段\n",
    "\n",
    "            # 构建输出序列\n",
    "            output_ids = tokenizer.encode_plus(\n",
    "                output_text,\n",
    "                return_tensors = \"pt\",\n",
    "                max_length = tokenizer.model_max_length,\n",
    "                truncation = True,\n",
    "                padding = False,\n",
    "            ).input_ids\n",
    "\n",
    "            # 拼接输入与输出序列，获得模型所需的input_ids\n",
    "            full_input = torch.cat([input_ids, output_ids], dim=1)\n",
    "            inputs.append(full_input)\n",
    "\n",
    "            # 创建标签张量\n",
    "            labels_tensor = torch.full_like(full_input, -100) # 用-100填充表示这些位置在损失计算中被忽略\n",
    "            labels_tensor[:, input_ids.shape[1]:] = full_input[:, input_ids.shape[1]:] # 将output_ids对应的位置替换为output_ids原来的值，input_ids对应的位置仍为-100，表示学习时只学习output部分\n",
    "            labels.append(labels_tensor)\n",
    "\n",
    "            if full_input.shape[1] > max_length:\n",
    "                max_length = full_input.shape[1]\n",
    "\n",
    "\n",
    "        # 处理batch的padding，将同一个batch的label和input都填充到同一个长度\n",
    "        inputs = [torch.nn.functional.pad(input, (0,max_length - input.size(1)),value=tokenizer.pad_token_id) for input in inputs]\n",
    "        labels = [torch.nn.functional.pad(label, (0, max_length - label.size(1)), value=-100) for label in labels]\n",
    "\n",
    "        inputs = torch.stack(inputs)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        # inputs = torch.cat(inputs, dim=0)\n",
    "        # labels = torch.cat(labels, dim=0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": (inputs != tokenizer.pad_token_id).to(dtype=torch.int), \n",
    "        }\n",
    "\n",
    "#data_collator(dataset['train'][:4])\n",
    "\n",
    "# batch = dataset['train'][:4]\n",
    "# print(batch)\n",
    "# batch_size = len(batch['input'])\n",
    "# for i in range(batch_size):\n",
    "#     print(batch['input'][i])\n",
    "#     print(batch['output'][i])\n",
    "#     print(batch['instruction'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 158])\n"
     ]
    }
   ],
   "source": [
    "# print(len(input_ids['label'])) # 51760\n",
    "# print(len(input_ids['input_ids']))\n",
    "# input_ids\n",
    "dataset = datasets.load_dataset('json', data_files=\"alpaca-cleaned/alpaca_data_cleaned.json\")\n",
    "dataset = dataset['train']\n",
    "sample = dataset[0]\n",
    "instruction = sample.get(\"instruction\",\"\")\n",
    "input_text = sample.get(\"input\",\"\")\n",
    "output_text = sample.get(\"output\",\"\")\n",
    "\n",
    "# 构建输入序列\n",
    "input_ids = tokenizer.encode_plus(\n",
    "f\"{instruction}{input_text}\", \n",
    "return_tensors = \"pt\", \n",
    "max_length = tokenizer.model_max_length,\n",
    "truncation = True, \n",
    "padding = False, \n",
    ").input_ids \n",
    "\n",
    "output_ids = tokenizer.encode_plus(\n",
    "f\"{output_text}\", \n",
    "return_tensors = \"pt\", \n",
    "max_length = tokenizer.model_max_length,\n",
    "truncation = True, \n",
    "padding = False, \n",
    ").input_ids \n",
    "\n",
    "print(input_ids.size())\n",
    "print(output_ids.size())\n",
    "\n",
    "full_input = torch.cat([input_ids, output_ids], dim=1)\n",
    "labels_tensor = torch.full_like(full_input, -100)\n",
    "print(full_input.size())\n",
    "print(labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.', 'instruction': 'Give three tips for staying healthy.', 'input': ''}\n",
      "{'output': 'The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).', 'instruction': 'What are the three primary colors?', 'input': ''}\n",
      "{'output': \"An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\\n\\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\\n\\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \\n\\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\", 'instruction': 'Describe the structure of an atom.', 'input': ''}\n",
      "{'output': 'There are several ways to reduce air pollution, including:\\n\\n1. Reduce energy consumption: By conserving energy, we reduce the amount of pollution emitted from power plants. Some ways to achieve this include: using energy-efficient appliances and lighting, insulating homes, and shutting off lights and electronics when not in use.\\n\\n2. Use public transportation or carpool: Cars and trucks are major sources of pollution. By using public transportation, carpooling or biking, you can reduce the number of cars on the road, which in turn reduces emissions.\\n\\n3. Be responsible with waste: When waste is burned or decomposed, they can release harmful gases into the air. Dispose of hazardous waste correctly and recycle materials like plastics, paper, and glass when possible.\\n\\n4. Support clean energy sources: Renewable energy sources like solar and wind power generate little or no pollution. Support clean energy companies and projects, and consider installing renewable energy systems in your home.\\n\\n5. Drive efficiently: If you need to drive, make sure your vehicle is well maintained and drive efficiently by avoiding rapid acceleration and braking, and reducing idling.\\n\\n6. Be mindful of product choices: Some products and practices release more pollution than others. Choose products with eco-friendly packaging and try to minimize your use of aerosol sprays, which can release harmful chemicals into the air.', 'instruction': 'How can we reduce air pollution?', 'input': ''}\n"
     ]
    }
   ],
   "source": [
    "# print(dataset[0])\n",
    "# print(dataset[1])\n",
    "# print(dataset[2])\n",
    "# print(dataset[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:42:58.207418Z",
     "iopub.status.busy": "2024-12-06T12:42:58.207100Z",
     "iopub.status.idle": "2024-12-06T12:42:58.218014Z",
     "shell.execute_reply": "2024-12-06T12:42:58.216987Z",
     "shell.execute_reply.started": "2024-12-06T12:42:58.207391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The main function\n",
    "# NOTE You can customize some logs to monitor your program.\n",
    "def finetune():\n",
    "    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))  # 解析器\n",
    "    # 返回模型参数，数据参数，训练参数\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    # TODO Step 2: Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "    dtype = torch.float16 if model_args.torch_dtype == \"float16\" else(\n",
    "        torch.bfloat16 if model_args.torch_dtype == \"bfloat16\" else torch.float32\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path)\n",
    "\n",
    "    # TODO Step 3: Load dataset\n",
    "    dataset = datasets.load_dataset('json', data_files=data_args.dataset_path)\n",
    "\n",
    "    \n",
    "    # TODO Step 4: Define the data collator function\n",
    "    def data_collator(batch: List[Dict]):\n",
    "        \"\"\"\n",
    "        batch: list of dict, each dict of the list is a sample in the dataset.\n",
    "        一个列表，每个元素是一个字典，字典是数据集中的一个样本\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        max_length = 0\n",
    "\n",
    "        for sample in batch:\n",
    "            instruction = sample.get(\"instruction\",\"\")\n",
    "            input_text = sample.get(\"input\",\"\")\n",
    "            output_text = sample.get(\"output\",\"\")\n",
    "\n",
    "            if not instruction.strip() and not input_text.strip() and not output_text.strip():\n",
    "                print(f\"发现无效的样本：instruction={instruction}, input={input_text}, output={output_text}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            if not instruction.strip():\n",
    "                print(\"instruction is empty\")\n",
    "                b = input()\n",
    "            # 检查input和output是否为空\n",
    "            if input_text.strip():\n",
    "                input_text = f\"\\n{input_text}\"\n",
    "\n",
    "            if not output_text.strip():\n",
    "                print(\"output is empty\")\n",
    "                c = input\n",
    "                output_text = \"<empty>\"\n",
    "            \n",
    "            # 构建输入序列\n",
    "            input_ids = tokenizer.encode_plus(\n",
    "                f\"{instruction}{input_text}\", # 将instruction和input_text进行拼接，生成文本输入\n",
    "                return_tensors = \"pt\", # 输出转换为pytorch的张量格式\n",
    "                max_length = tokenizer.model_max_length, # 如果输入序列超过最大长度则截断\n",
    "                truncation = True, \n",
    "                padding = False, # 即使输入序列没有达到最大长度，也不进行填充\n",
    "            ).input_ids # 用于获取tokenizer返回字典中的‘input_ids’字段\n",
    "\n",
    "            # 构建输出序列\n",
    "            output_ids = tokenizer.encode_plus(\n",
    "                output_text,\n",
    "                return_tensors = \"pt\",\n",
    "                max_length = tokenizer.model_max_length,\n",
    "                truncation = True,\n",
    "                padding = False,\n",
    "            ).input_ids\n",
    "\n",
    "            if len(input_ids) == 0 or len(output_ids) == 0:\n",
    "                print(f\"发现空样本: instruction={instruction}, input={input_text}, output={output_text}\")\n",
    "                continue\n",
    "\n",
    "            # 拼接输入与输出序列，获得模型所需的input_ids\n",
    "            full_input = torch.cat([input_ids, output_ids], dim=1)\n",
    "            inputs.append(full_input)\n",
    "\n",
    "            # 创建标签张量\n",
    "            labels_tensor = torch.full_like(full_input, -100) # 用-100填充表示这些位置在损失计算中被忽略\n",
    "            labels_tensor[:, input_ids.shape[1]:] = full_input[:, input_ids.shape[1]:] # 将output_ids对应的位置替换为output_ids原来的值，input_ids对应的位置仍为-100，表示学习时只学习output部分\n",
    "            labels.append(labels_tensor)\n",
    "\n",
    "            if full_input.shape[1] > max_length:\n",
    "                max_length = full_input.shape[1]\n",
    "\n",
    "\n",
    "        # 处理batch的padding，将同一个batch的label和input都填充到同一个长度\n",
    "        # inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        # labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        inputs = [torch.nn.functional.pad(input, (0,max_length - input.size(1)),value=tokenizer.pad_token_id) for input in inputs]\n",
    "        labels = [torch.nn.functional.pad(label, (0, max_length - label.size(1)), value=-100) for label in labels]\n",
    "\n",
    "        # inputs = torch.stack(inputs)\n",
    "        # labels = torch.stack(labels)\n",
    "\n",
    "        inputs = torch.cat(inputs, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "        print(inputs.size())\n",
    "        print(labels.size())\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": torch.ne(inputs, tokenizer.pad_token_id).int(), \n",
    "            # pad_token_id是tokenizer定义的填充令牌ID，也就是对padding的部分填充一个特殊的令牌\n",
    "            # 该行代码将生成一个与inputs张量形状相同的布尔张量，其中值为True:表示对应的输入ID不是填充ID（即该令牌是有效的；值为False: 表示对应的输入ID是填充ID（即该令牌是无效的，需要被忽略）。\n",
    "        }\n",
    "        # \"attention_mask\": torch.tensor(inputs != tokenizer.pad_token_id).to(dtype=torch.int)\n",
    "\n",
    "    # TODO Step 5: Define the Trainer\n",
    "    # HINT: https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "    trainer = Trainer(\n",
    "        args = training_args,\n",
    "        model=model,  # Pretrained model\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "    )\n",
    "\n",
    "    # Step 6: Train!\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1113461/1176994480.py:112: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 166])\n",
      "torch.Size([6, 166])\n",
      "torch.Size([6, 234])\n",
      "torch.Size([6, 234])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "chunk expects at least a 1-dimensional tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# max_length = max([label.size() for label in labels])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotebook\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--output_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./our-model/test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# \"--local_rank\",\"-1\",\u001b[39;00m\n\u001b[1;32m     23\u001b[0m ]\n\u001b[0;32m---> 24\u001b[0m \u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# training_args = TrainingArguments(\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#     output_dir=output_dir,\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     per_device_train_batch_size=2,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#     #dataset_text_field=\"text\",\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 121\u001b[0m, in \u001b[0;36mfinetune\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    113\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_args,\n\u001b[1;32m    114\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,  \u001b[38;5;66;03m# Pretrained model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Step 6: Train!\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:183\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule must have its parameters and buffers \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (device_ids[0]) but found one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         )\n\u001b[0;32m--> 183\u001b[0m inputs, module_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# for forward function without any inputs, empty list and dict will be created\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# so the module can be executed on one device which is the first one in device_ids\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inputs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_kwargs:\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:207\u001b[0m, in \u001b[0;36mDataParallel.scatter\u001b[0;34m(self, inputs, kwargs, device_ids)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    203\u001b[0m     inputs: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[1;32m    204\u001b[0m     kwargs: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m    205\u001b[0m     device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]],\n\u001b[1;32m    206\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:89\u001b[0m, in \u001b[0;36mscatter_kwargs\u001b[0;34m(inputs, kwargs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Scatter with support for kwargs dictionary.\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m scattered_inputs \u001b[38;5;241m=\u001b[39m scatter(inputs, target_gpus, dim) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m---> 89\u001b[0m scattered_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scattered_inputs) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(scattered_kwargs):\n\u001b[1;32m     91\u001b[0m     scattered_inputs\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m     92\u001b[0m         () \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(scattered_kwargs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(scattered_inputs))\n\u001b[1;32m     93\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(inputs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# After scatter_map is called, a scatter_map cell will exist. This cell\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# has a reference to the actual function scatter_map, which has references\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# to a closure that has a reference to the scatter_map cell (because the\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# fn is recursive). To avoid this reference cycle, we set the function to\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# None, clearing the cell\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mscatter_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     scatter_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:66\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscatter_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [obj \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m target_gpus]\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:62\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscatter_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:58\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter_map\u001b[39m(obj):\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mScatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(obj):\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:104\u001b[0m, in \u001b[0;36mScatter.forward\u001b[0;34m(ctx, target_gpus, chunk_sizes, dim, input)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Perform CPU to GPU copies in a background stream\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     streams \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    102\u001b[0m         _get_stream(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)) \u001b[38;5;28;01mfor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m target_gpus\n\u001b[1;32m    103\u001b[0m     ]\n\u001b[0;32m--> 104\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Synchronize with the copy stream\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/yyhw/lib/python3.10/site-packages/torch/nn/parallel/comm.py:205\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(tensor, devices, chunk_sizes, dim, streams, out)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m devices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: chunk expects at least a 1-dimensional tensor"
     ]
    }
   ],
   "source": [
    "# Pass your training arguments.\n",
    "# NOTE [IMPORTANT!!!] DO NOT FORGET TO PASS PROPER ARGUMENTS TO SAVE YOUR CHECKPOINTS!!!\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4,5\"\n",
    "# max_length = max([label.size() for label in labels])\n",
    "sys.argv = [\n",
    "    \"notebook\", \n",
    "    \"--output_dir\", \"./our-model/test\",\n",
    "    \"--learning_rate\",\"1e-5\",\n",
    "    \"--num_train_epochs\", \"3\",  # 通常3-5个epoch即可收敛，长时间训练可能会过拟合\n",
    "    \"--per_device_train_batch_size\", \"1\",  # 每个GPU上的大小\n",
    "    \"--overwrite_output_dir\",\"True\",  #开发过程中覆盖旧的文件\n",
    "    \"--save_steps\", \"1000\",\n",
    "    \"--save_total_limit\", \"2\",\n",
    "    \"--logging_steps\",\"50\",\n",
    "    \"--logging_dir\", \"./logs/exp1\",\n",
    "    \"--dataloader_drop_last\", \"True\",\n",
    "    '--seed','42',\n",
    "    \"--fp16\",\"True\",\n",
    "    '--remove_unused_columns','False',\n",
    "    \"--ddp_find_unused_parameters\",\"False\",  # 关闭未使用的参数检查\n",
    "    # \"--local_rank\",\"-1\",\n",
    "]\n",
    "finetune()\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     learning_rate=2e-4,\n",
    "#     logging_steps=2,\n",
    "#     max_steps=100,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     ddp_find_unused_parameters=False,\n",
    "#     #dataset_text_field=\"text\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:43:09.552702Z",
     "iopub.status.busy": "2024-12-06T12:43:09.552042Z",
     "iopub.status.idle": "2024-12-06T12:46:03.173893Z",
     "shell.execute_reply": "2024-12-06T12:46:03.172922Z",
     "shell.execute_reply.started": "2024-12-06T12:43:09.552665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_967154/702490210.py:125: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38820' max='38820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38820/38820 3:06:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.407400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.484700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.400900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.417700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.436600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.445900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.446800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.446400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.435400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.456900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.460900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.430100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.402700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.445300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.432700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.443200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.423800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.438800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.418400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.401400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.478900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.395500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.407800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.447100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.458200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.403800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.421700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.422600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.448100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.408900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.398900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.388900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>1.426600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.415600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.423300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.519400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.393100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.465200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.445600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>1.465500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>1.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.427600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>1.385200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.431500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>1.375200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>1.363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.425500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>1.438300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.391700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>1.459700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>1.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>1.459300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>1.395100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>1.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>1.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>1.435900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>1.354100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>1.525800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.396500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>1.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.419200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>1.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>1.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>1.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>1.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.423800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>1.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>1.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.476700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>1.328300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.392900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>1.472200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>1.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>1.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.411800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>1.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.477800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>1.405600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>1.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>1.347800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.380500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>1.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.433000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>1.395300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>1.405200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>1.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.457200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>1.508800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>1.442800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>1.481400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.419700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>1.349900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>1.411600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>1.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>1.450500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>1.437100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>1.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.438800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>1.397500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>1.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>1.345400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.428800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>1.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>1.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.442600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>1.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.421200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>1.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.445800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>1.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>1.405200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.458300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>1.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.470400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>1.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.339800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>1.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>1.400100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>1.451900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.452300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>1.370800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>1.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>1.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>1.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.432700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>1.466200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.309300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>1.377700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>1.371100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.424500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>1.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.390700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>1.348100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.415900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>1.410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>1.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.416700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>1.393100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.434200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>1.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>1.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.440700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>1.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>1.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>1.350500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>1.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.373900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>1.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.386600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>1.321700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>1.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12650</td>\n",
       "      <td>1.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>1.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>1.454400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>1.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12850</td>\n",
       "      <td>1.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>1.403400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12950</td>\n",
       "      <td>1.290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.960600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13050</td>\n",
       "      <td>0.997300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13150</td>\n",
       "      <td>0.967600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.971900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>0.989600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13350</td>\n",
       "      <td>0.973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>1.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13450</td>\n",
       "      <td>0.940600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.980200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13550</td>\n",
       "      <td>0.991100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13650</td>\n",
       "      <td>1.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>1.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>0.982400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>1.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13850</td>\n",
       "      <td>0.989300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.973800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13950</td>\n",
       "      <td>1.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14050</td>\n",
       "      <td>1.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>1.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14150</td>\n",
       "      <td>1.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.984400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>0.952600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.949300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14350</td>\n",
       "      <td>1.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>1.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14450</td>\n",
       "      <td>0.979400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14550</td>\n",
       "      <td>0.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>1.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14650</td>\n",
       "      <td>0.977900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.990500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>0.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.950600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14850</td>\n",
       "      <td>1.040300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.989800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14950</td>\n",
       "      <td>0.978400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.978100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15050</td>\n",
       "      <td>0.983200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>1.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15150</td>\n",
       "      <td>0.988400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>1.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15350</td>\n",
       "      <td>0.995900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>1.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15450</td>\n",
       "      <td>0.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15550</td>\n",
       "      <td>0.987800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>1.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15650</td>\n",
       "      <td>0.975100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15750</td>\n",
       "      <td>0.976700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15850</td>\n",
       "      <td>1.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15950</td>\n",
       "      <td>0.962100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16050</td>\n",
       "      <td>1.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.969200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16150</td>\n",
       "      <td>0.983100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>1.035700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16250</td>\n",
       "      <td>0.990300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>1.034600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16350</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16450</td>\n",
       "      <td>0.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16550</td>\n",
       "      <td>0.974800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>1.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16650</td>\n",
       "      <td>1.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.966800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16750</td>\n",
       "      <td>1.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>1.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16850</td>\n",
       "      <td>1.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16950</td>\n",
       "      <td>0.962200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.967500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17050</td>\n",
       "      <td>0.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>1.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17150</td>\n",
       "      <td>0.980100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>1.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17250</td>\n",
       "      <td>0.994100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>1.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17350</td>\n",
       "      <td>0.979200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.989200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17450</td>\n",
       "      <td>1.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.965800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17550</td>\n",
       "      <td>0.940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>1.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17650</td>\n",
       "      <td>1.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>1.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17750</td>\n",
       "      <td>1.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.966800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17850</td>\n",
       "      <td>1.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>1.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17950</td>\n",
       "      <td>0.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18050</td>\n",
       "      <td>1.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>1.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18150</td>\n",
       "      <td>1.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.929500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18250</td>\n",
       "      <td>0.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18350</td>\n",
       "      <td>1.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18450</td>\n",
       "      <td>0.985700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18550</td>\n",
       "      <td>1.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18650</td>\n",
       "      <td>1.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>1.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18750</td>\n",
       "      <td>0.998000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>1.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18850</td>\n",
       "      <td>0.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>1.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18950</td>\n",
       "      <td>1.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19050</td>\n",
       "      <td>0.975100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>1.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19150</td>\n",
       "      <td>0.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.981900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19250</td>\n",
       "      <td>1.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.998200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19350</td>\n",
       "      <td>1.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.990600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19450</td>\n",
       "      <td>1.065900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19550</td>\n",
       "      <td>0.990800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>1.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19650</td>\n",
       "      <td>1.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.963700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19750</td>\n",
       "      <td>1.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>1.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19850</td>\n",
       "      <td>1.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>1.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19950</td>\n",
       "      <td>0.980300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20050</td>\n",
       "      <td>0.975800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>1.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20150</td>\n",
       "      <td>0.951200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>1.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20250</td>\n",
       "      <td>1.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>1.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20350</td>\n",
       "      <td>1.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>1.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20450</td>\n",
       "      <td>1.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.984400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20550</td>\n",
       "      <td>1.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>1.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20650</td>\n",
       "      <td>1.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>1.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20750</td>\n",
       "      <td>1.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>1.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20850</td>\n",
       "      <td>0.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20950</td>\n",
       "      <td>0.973100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.995900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21050</td>\n",
       "      <td>1.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21150</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.947400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21250</td>\n",
       "      <td>0.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.978700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21350</td>\n",
       "      <td>0.981900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>1.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21450</td>\n",
       "      <td>0.967500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.949700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21550</td>\n",
       "      <td>1.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21650</td>\n",
       "      <td>0.983600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21750</td>\n",
       "      <td>1.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21850</td>\n",
       "      <td>0.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21950</td>\n",
       "      <td>0.967900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22050</td>\n",
       "      <td>0.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22150</td>\n",
       "      <td>1.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.954300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22250</td>\n",
       "      <td>1.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22350</td>\n",
       "      <td>1.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.958100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22450</td>\n",
       "      <td>0.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.935800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22550</td>\n",
       "      <td>1.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22650</td>\n",
       "      <td>0.965600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>0.965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22750</td>\n",
       "      <td>1.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.954500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22850</td>\n",
       "      <td>1.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>1.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22950</td>\n",
       "      <td>0.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23050</td>\n",
       "      <td>1.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23150</td>\n",
       "      <td>0.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23250</td>\n",
       "      <td>1.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>0.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23350</td>\n",
       "      <td>0.996800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23450</td>\n",
       "      <td>1.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>1.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23550</td>\n",
       "      <td>1.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.935900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23650</td>\n",
       "      <td>0.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>1.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23750</td>\n",
       "      <td>1.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>0.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23850</td>\n",
       "      <td>0.930400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>1.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23950</td>\n",
       "      <td>1.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24050</td>\n",
       "      <td>1.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>1.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24150</td>\n",
       "      <td>0.995800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>1.031500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24250</td>\n",
       "      <td>1.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>0.988100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24350</td>\n",
       "      <td>1.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>1.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24450</td>\n",
       "      <td>0.988400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.981900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24550</td>\n",
       "      <td>0.996200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>1.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24650</td>\n",
       "      <td>1.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>1.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24750</td>\n",
       "      <td>0.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24850</td>\n",
       "      <td>0.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>1.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24950</td>\n",
       "      <td>0.957300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25050</td>\n",
       "      <td>1.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25100</td>\n",
       "      <td>1.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25150</td>\n",
       "      <td>0.982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>0.988700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25250</td>\n",
       "      <td>1.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25300</td>\n",
       "      <td>1.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25350</td>\n",
       "      <td>0.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>0.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25450</td>\n",
       "      <td>1.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.966700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25550</td>\n",
       "      <td>1.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>0.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25650</td>\n",
       "      <td>1.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25700</td>\n",
       "      <td>1.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25750</td>\n",
       "      <td>1.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>0.941500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25850</td>\n",
       "      <td>1.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25900</td>\n",
       "      <td>0.925600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25950</td>\n",
       "      <td>0.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.692200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26050</td>\n",
       "      <td>0.677900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26100</td>\n",
       "      <td>0.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26150</td>\n",
       "      <td>0.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>0.702800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26250</td>\n",
       "      <td>0.675500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26300</td>\n",
       "      <td>0.686600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26350</td>\n",
       "      <td>0.649600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>0.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26450</td>\n",
       "      <td>0.637600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26550</td>\n",
       "      <td>0.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>0.651300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26650</td>\n",
       "      <td>0.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26700</td>\n",
       "      <td>0.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26750</td>\n",
       "      <td>0.667600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>0.737900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26850</td>\n",
       "      <td>0.637100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26900</td>\n",
       "      <td>0.712600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26950</td>\n",
       "      <td>0.677800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27050</td>\n",
       "      <td>0.647200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27100</td>\n",
       "      <td>0.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27150</td>\n",
       "      <td>0.641900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>0.661400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27250</td>\n",
       "      <td>0.664100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27300</td>\n",
       "      <td>0.659500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27350</td>\n",
       "      <td>0.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>0.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27450</td>\n",
       "      <td>0.685300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.647300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27550</td>\n",
       "      <td>0.714900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27650</td>\n",
       "      <td>0.646600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27700</td>\n",
       "      <td>0.637100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27750</td>\n",
       "      <td>0.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>0.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27850</td>\n",
       "      <td>0.660100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27900</td>\n",
       "      <td>0.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27950</td>\n",
       "      <td>0.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.709400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28050</td>\n",
       "      <td>0.678500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28100</td>\n",
       "      <td>0.653300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28150</td>\n",
       "      <td>0.698900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>0.728100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28250</td>\n",
       "      <td>0.638600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28300</td>\n",
       "      <td>0.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28350</td>\n",
       "      <td>0.712700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>0.624100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28450</td>\n",
       "      <td>0.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.634400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28550</td>\n",
       "      <td>0.685700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>0.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28650</td>\n",
       "      <td>0.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28700</td>\n",
       "      <td>0.654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28750</td>\n",
       "      <td>0.631300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>0.665500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28850</td>\n",
       "      <td>0.649600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28900</td>\n",
       "      <td>0.692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28950</td>\n",
       "      <td>0.661200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.613200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29050</td>\n",
       "      <td>0.658200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29100</td>\n",
       "      <td>0.678600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29150</td>\n",
       "      <td>0.678600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>0.623800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29250</td>\n",
       "      <td>0.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29300</td>\n",
       "      <td>0.715600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29350</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>0.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29450</td>\n",
       "      <td>0.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29550</td>\n",
       "      <td>0.652500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>0.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29650</td>\n",
       "      <td>0.630200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29700</td>\n",
       "      <td>0.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29750</td>\n",
       "      <td>0.714300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>0.644700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29850</td>\n",
       "      <td>0.692200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29900</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29950</td>\n",
       "      <td>0.657300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.716700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30050</td>\n",
       "      <td>0.648200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30100</td>\n",
       "      <td>0.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30150</td>\n",
       "      <td>0.646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>0.669900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30250</td>\n",
       "      <td>0.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30300</td>\n",
       "      <td>0.696900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30350</td>\n",
       "      <td>0.707200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30450</td>\n",
       "      <td>0.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30550</td>\n",
       "      <td>0.646800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30650</td>\n",
       "      <td>0.681100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30700</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30750</td>\n",
       "      <td>0.662600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>0.691200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30850</td>\n",
       "      <td>0.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30900</td>\n",
       "      <td>0.656500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30950</td>\n",
       "      <td>0.706200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31050</td>\n",
       "      <td>0.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31100</td>\n",
       "      <td>0.647900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31150</td>\n",
       "      <td>0.666100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>0.649900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31250</td>\n",
       "      <td>0.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31300</td>\n",
       "      <td>0.677800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31350</td>\n",
       "      <td>0.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>0.687200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31450</td>\n",
       "      <td>0.726800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31550</td>\n",
       "      <td>0.638200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>0.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31650</td>\n",
       "      <td>0.715400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31700</td>\n",
       "      <td>0.703300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31750</td>\n",
       "      <td>0.706400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>0.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31850</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31900</td>\n",
       "      <td>0.601100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31950</td>\n",
       "      <td>0.706200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32050</td>\n",
       "      <td>0.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32100</td>\n",
       "      <td>0.701200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32150</td>\n",
       "      <td>0.637900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32250</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32300</td>\n",
       "      <td>0.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32350</td>\n",
       "      <td>0.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>0.745400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32450</td>\n",
       "      <td>0.691200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.644400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32550</td>\n",
       "      <td>0.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>0.682400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32650</td>\n",
       "      <td>0.645900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32700</td>\n",
       "      <td>0.680500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32750</td>\n",
       "      <td>0.638700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>0.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32850</td>\n",
       "      <td>0.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32900</td>\n",
       "      <td>0.717900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32950</td>\n",
       "      <td>0.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.670600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33050</td>\n",
       "      <td>0.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33100</td>\n",
       "      <td>0.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33150</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>0.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33250</td>\n",
       "      <td>0.690700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33300</td>\n",
       "      <td>0.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33350</td>\n",
       "      <td>0.632200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>0.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33450</td>\n",
       "      <td>0.668200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33550</td>\n",
       "      <td>0.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>0.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33650</td>\n",
       "      <td>0.720300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33700</td>\n",
       "      <td>0.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33750</td>\n",
       "      <td>0.620400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>0.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33850</td>\n",
       "      <td>0.676200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33900</td>\n",
       "      <td>0.658400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33950</td>\n",
       "      <td>0.641600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.665700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34050</td>\n",
       "      <td>0.664400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34100</td>\n",
       "      <td>0.654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34150</td>\n",
       "      <td>0.628400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>0.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34250</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34300</td>\n",
       "      <td>0.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34350</td>\n",
       "      <td>0.672900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>0.666800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34450</td>\n",
       "      <td>0.611700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34550</td>\n",
       "      <td>0.664100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>0.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34650</td>\n",
       "      <td>0.632600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34700</td>\n",
       "      <td>0.663700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34750</td>\n",
       "      <td>0.685700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>0.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34850</td>\n",
       "      <td>0.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34900</td>\n",
       "      <td>0.669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34950</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35050</td>\n",
       "      <td>0.639100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35100</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35150</td>\n",
       "      <td>0.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>0.691700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35250</td>\n",
       "      <td>0.642200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35300</td>\n",
       "      <td>0.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35350</td>\n",
       "      <td>0.740500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>0.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35450</td>\n",
       "      <td>0.649600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35550</td>\n",
       "      <td>0.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>0.675700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35650</td>\n",
       "      <td>0.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35700</td>\n",
       "      <td>0.681100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35750</td>\n",
       "      <td>0.646600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>0.609400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35850</td>\n",
       "      <td>0.676400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35900</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35950</td>\n",
       "      <td>0.651400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.624200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36050</td>\n",
       "      <td>0.669900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36100</td>\n",
       "      <td>0.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36150</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>0.653100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36250</td>\n",
       "      <td>0.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36300</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36350</td>\n",
       "      <td>0.699500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>0.657300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36450</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36550</td>\n",
       "      <td>0.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>0.655800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36650</td>\n",
       "      <td>0.627900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36700</td>\n",
       "      <td>0.656300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36750</td>\n",
       "      <td>0.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>0.696600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36850</td>\n",
       "      <td>0.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36900</td>\n",
       "      <td>0.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36950</td>\n",
       "      <td>0.639500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37050</td>\n",
       "      <td>0.644700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37100</td>\n",
       "      <td>0.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37150</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>0.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37250</td>\n",
       "      <td>0.707900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37300</td>\n",
       "      <td>0.682700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37350</td>\n",
       "      <td>0.709800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37400</td>\n",
       "      <td>0.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37450</td>\n",
       "      <td>0.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37550</td>\n",
       "      <td>0.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37600</td>\n",
       "      <td>0.655900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37650</td>\n",
       "      <td>0.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37700</td>\n",
       "      <td>0.709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37750</td>\n",
       "      <td>0.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>0.690100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37850</td>\n",
       "      <td>0.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37900</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37950</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.700900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38050</td>\n",
       "      <td>0.683300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38100</td>\n",
       "      <td>0.699600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38150</td>\n",
       "      <td>0.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38200</td>\n",
       "      <td>0.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38250</td>\n",
       "      <td>0.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38300</td>\n",
       "      <td>0.691500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38350</td>\n",
       "      <td>0.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>0.676600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38450</td>\n",
       "      <td>0.751500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38550</td>\n",
       "      <td>0.673200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38600</td>\n",
       "      <td>0.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38650</td>\n",
       "      <td>0.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38700</td>\n",
       "      <td>0.714500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38750</td>\n",
       "      <td>0.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38800</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pass your training arguments.\n",
    "# NOTE [IMPORTANT!!!] DO NOT FORGET TO PASS PROPER ARGUMENTS TO SAVE YOUR CHECKPOINTS!!!\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.argv = [\n",
    "    \"notebook\", \n",
    "    \"--output_dir\", \"./our-model/test\",\n",
    "    \"--learning_rate\",\"1e-5\",\n",
    "    \"--num_train_epochs\", \"3\",  # 通常3-5个epoch即可收敛，长时间训练可能会过拟合\n",
    "    \"--per_device_train_batch_size\", \"4\",  # 每个GPU上的大小\n",
    "    \"--overwrite_output_dir\",\"True\",  #开发过程中覆盖旧的文件\n",
    "    \"--save_steps\", \"1000\",\n",
    "    \"--save_total_limit\", \"2\",\n",
    "    \"--logging_steps\",\"50\",\n",
    "    \"--logging_dir\", \"./logs/exp1\",\n",
    "    \"--remove_unused_columns\",\"False\",\n",
    "    \"--dataloader_drop_last\", \"True\",\n",
    "    '--seed','42',\n",
    "    \"--fp16\",\"True\",\n",
    "    # \"--local_rank\",\"-1\",\n",
    "]\n",
    "finetune()\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     learning_rate=2e-4,\n",
    "#     logging_steps=2,\n",
    "#     max_steps=100,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     ddp_find_unused_parameters=False,\n",
    "#     #dataset_text_field=\"text\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 原始预训练模型\n",
    "PLM_MODEL_PATH = \"./Qwen2.5-0.5B\"\n",
    "# 微调后的模型\n",
    "SFT_MODEL_PATH = \"./our-model/dev/checkpoint-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有多个GPU，可以修改下面的--hf-num-gpus参数来加速评测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!opencompass \\\n",
    "    --datasets mmlu_ppl hellaswag_clean_ppl winogrande_ll ARC_e_ppl ARC_c_clean_ppl SuperGLUE_BoolQ_few_shot_ppl \\\n",
    "    --summarizer example \\\n",
    "    --hf-type base \\\n",
    "    --hf-path \"./Qwen2.5-0.5B\"\\\n",
    "    --tokenizer-kwargs padding_side=\"left\" truncation=\"left\" \\\n",
    "    --max-seq-len 2048 \\\n",
    "    --batch-size 4 \\\n",
    "    --hf-num-gpus 6\\\n",
    "    --work-dir \"outputs/evals/plm\" \\\n",
    "    --debug\n",
    "\n",
    "# --hf-num-gpus 6 原来为2\n",
    "# python opencompass/run.py --datasets mmlu_ppl hellaswag_clean_ppl winogrande_ll ARC_e_ppl ARC_c_clean_ppl SuperGLUE_BoolQ_few_shot_ppl --summarizer example --hf-type base --hf-path \"./Qwen2.5-0.5B\" --tokenizer-kwargs padding_side=\"left\" truncation=\"left\"  --max-seq-len 2048 --batch-size 4 --hf-num-gpus 6 --work-dir \"outputs/evals/plm\" --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!opencompass \\\n",
    "    --datasets mmlu_ppl hellaswag_clean_ppl winogrande_ll ARC_e_ppl ARC_c_clean_ppl SuperGLUE_BoolQ_few_shot_ppl \\\n",
    "    --summarizer example \\\n",
    "    --hf-type base \\\n",
    "    --hf-path {SFT_MODEL_PATH} \\\n",
    "    --tokenizer-kwargs padding_side=\"left\" truncation=\"left\" \\\n",
    "    --max-seq-len 2048 \\\n",
    "    --batch-size 4 \\\n",
    "    --hf-num-gpus 6 \\\n",
    "    --work-dir \"outputs/evals/sft\" \\\n",
    "    --debug\n",
    "\n",
    "    # python opencompass/run.py --datasets mmlu_ppl hellaswag_clean_ppl winogrande_ll ARC_e_ppl ARC_c_clean_ppl SuperGLUE_BoolQ_few_shot_ppl --summarizer example --hf-type base --hf-path \"./our-model/test/checkpoint-25878\" --tokenizer-kwargs padding_side=\"left\" truncation=\"left\"  --max-seq-len 2048 --batch-size 4 --hf-num-gpus 6 --work-dir \"outputs/evals/sft\" --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()  # 清理 Python 内存\n",
    "torch.cuda.empty_cache()  # 释放 CUDA 资源\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a =torch.tensor([1,2,3])\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4061777,
     "sourceId": 7056498,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164048,
     "modelInstanceId": 141432,
     "sourceId": 166218,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "yyhw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
