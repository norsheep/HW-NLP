{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装opencompass：Kaggle上已经为我们准备好了其他常用包，只需安装opencompass用于评测即可。如果不在Kaggle上运行，则还需要安装其他必要包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install \"opencompass[full]\"\n",
    "# !pip install pytorch transformers datasets \"opencompass[full]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指令微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:42:50.738461Z",
     "iopub.status.busy": "2024-12-06T12:42:50.738204Z",
     "iopub.status.idle": "2024-12-06T12:42:50.743023Z",
     "shell.execute_reply": "2024-12-06T12:42:50.742025Z",
     "shell.execute_reply.started": "2024-12-06T12:42:50.738434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The main program for finetuning LLMs with Huggingface Transformers Library.\n",
    "\n",
    "ALL SECTIONS WHERE CODE POSSIBLY NEEDS TO BE FILLED IN ARE MARKED AS TODO.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict\n",
    "import sys\n",
    "import torch\n",
    "from transformers import TrainingArguments, HfArgumentParser, Trainer, AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:42:54.214990Z",
     "iopub.status.busy": "2024-12-06T12:42:54.214309Z",
     "iopub.status.idle": "2024-12-06T12:42:54.220805Z",
     "shell.execute_reply": "2024-12-06T12:42:54.219860Z",
     "shell.execute_reply.started": "2024-12-06T12:42:54.214952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the arguments required for the main program.\n",
    "# NOTE: You can customize any arguments you need to pass in.\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"Arguments for model\n",
    "    \"\"\"\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the LLM to fine-tune or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype.\"\n",
    "            ),\n",
    "            \"choices\": [\"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    # TODO: add your model arguments here\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"Arguments for data\n",
    "    \"\"\"\n",
    "    dataset_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the fine-tuning dataset or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    # TODO: add your data arguments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:42:58.207418Z",
     "iopub.status.busy": "2024-12-06T12:42:58.207100Z",
     "iopub.status.idle": "2024-12-06T12:42:58.218014Z",
     "shell.execute_reply": "2024-12-06T12:42:58.216987Z",
     "shell.execute_reply.started": "2024-12-06T12:42:58.207391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The main function\n",
    "# NOTE You can customize some logs to monitor your program.\n",
    "def finetune():\n",
    "    # TODO Step 1: Define an arguments parser and parse the arguments\n",
    "    # NOTE Three parts: model arguments, data arguments, and training arguments\n",
    "    # HINT: Refer to \n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/internal/trainer_utils#transformers.HfArgumentParser\n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/main_classes/trainer#transformers.TrainingArguments\n",
    "    parser = ...\n",
    "    model_args, data_args, training_args = ...\n",
    "\n",
    "    # TODO Step 2: Load tokenizer and model\n",
    "    # HINT 1: Refer to\n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/main_classes/tokenizer#tokenizer\n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/qwen2\n",
    "    # HINT 2: To save training GPU memory, you need to set the model's parameter precision to half-precision (float16 or bfloat16).\n",
    "    #         You may also check other strategies to save the memory!\n",
    "    #   * https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/llama2#usage-tips\n",
    "    #   * https://huggingface.co/docs/transformers/perf_train_gpu_one\n",
    "    #   * https://www.53ai.com/news/qianyanjishu/2024052494875.html\n",
    "    tokenizer = ...\n",
    "    model = ...\n",
    "\n",
    "    # TODO Step 3: Load dataset\n",
    "    # HINT: https://huggingface.co/docs/datasets/v3.1.0/en/package_reference/main_classes#datasets.Dataset\n",
    "    dataset = ...\n",
    "\n",
    "    # TODO Step 4: Define the data collator function\n",
    "    # NOTE During training, for each model parameter update, we fetch a batch of data, perform a forward and backward pass,\n",
    "    # and then update the model parameters. The role of the data collator is to process the data (e.g., padding the data within\n",
    "    # a batch to the same length) and format the batch into the input required by the model.\n",
    "    #\n",
    "    # In this assignment, the purpose of the custom data_collator is to process each batch of data from the dataset loaded in\n",
    "    # Step 3 into the format required by the model. This includes tasks such as tokenizing the data, converting each token into \n",
    "    # an ID sequence, applying padding, and preparing labels.\n",
    "    # \n",
    "    # HINT:\n",
    "    #   * Before implementation, you should:\n",
    "    #      1. Clearly understand the format of each sample in the dataset loaded in Step 3.\n",
    "    #      2. Understand the input format required by the model (https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2ForCausalLM).\n",
    "    #         Reading its source code also helps!\n",
    "\n",
    "    def data_collator(batch: List[Dict]):\n",
    "        \"\"\"\n",
    "        batch: list of dict, each dict of the list is a sample in the dataset.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    # TODO Step 5: Define the Trainer\n",
    "    # HINT: https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "    trainer = Trainer(\n",
    "        ...,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Step 6: Train!\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:43:09.552702Z",
     "iopub.status.busy": "2024-12-06T12:43:09.552042Z",
     "iopub.status.idle": "2024-12-06T12:46:03.173893Z",
     "shell.execute_reply": "2024-12-06T12:46:03.172922Z",
     "shell.execute_reply.started": "2024-12-06T12:43:09.552665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pass your training arguments.\n",
    "# NOTE [IMPORTANT!!!] DO NOT FORGET TO PASS PROPER ARGUMENTS TO SAVE YOUR CHECKPOINTS!!!\n",
    "sys.argv = [\n",
    "    \"notebook\", \n",
    "    \"--arg1\", \"value1\",\n",
    "    \"--arg2\", \"value2\",\n",
    "    ...\n",
    "]\n",
    "finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PLM_MODEL_PATH = \"./Qwen2.5-0.5B\"\n",
    "SFT_MODEL_PATH = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有多个GPU，可以修改下面的--hf-num-gpus参数来加速评测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!opencompass \\\n",
    "    --datasets mmlu_ppl hellaswag_clean_ppl winogrande_ll ARC_e_ppl ARC_c_clean_ppl SuperGLUE_BoolQ_few_shot_ppl \\\n",
    "    --summarizer example \\\n",
    "    --hf-type base \\\n",
    "    --hf-path {PLM_MODEL_PATH} \\\n",
    "    --tokenizer-kwargs padding_side=\"left\" truncation=\"left\" \\\n",
    "    --max-seq-len 2048 \\\n",
    "    --batch-size 4 \\\n",
    "    --hf-num-gpus 2 \\\n",
    "    --work-dir \"outputs/evals/plm\" \\\n",
    "    --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!opencompass \\\n",
    "    --datasets mmlu_ppl hellaswag_clean_ppl winogrande_ll ARC_e_ppl ARC_c_clean_ppl SuperGLUE_BoolQ_few_shot_ppl \\\n",
    "    --summarizer example \\\n",
    "    --hf-type base \\\n",
    "    --hf-path {SFT_MODEL_PATH} \\\n",
    "    --tokenizer-kwargs padding_side=\"left\" truncation=\"left\" \\\n",
    "    --max-seq-len 2048 \\\n",
    "    --batch-size 4 \\\n",
    "    --hf-num-gpus 2 \\\n",
    "    --work-dir \"outputs/evals/sft\" \\\n",
    "    --debug"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4061777,
     "sourceId": 7056498,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164048,
     "modelInstanceId": 141432,
     "sourceId": 166218,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
