{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-28T13:44:17.713176Z",
     "iopub.status.busy": "2024-12-28T13:44:17.712969Z",
     "iopub.status.idle": "2024-12-28T13:45:30.465397Z",
     "shell.execute_reply": "2024-12-28T13:45:30.464069Z",
     "shell.execute_reply.started": "2024-12-28T13:44:17.713156Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install \"opencompass[full]\"\n",
    "\n",
    "!pip install --upgrade Pillow\n",
    "\n",
    "!pip install huggingface_hub==0.27.0\n",
    "\n",
    "!pip install peft==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:45:30.467728Z",
     "iopub.status.busy": "2024-12-28T13:45:30.467280Z",
     "iopub.status.idle": "2024-12-28T13:45:44.123943Z",
     "shell.execute_reply": "2024-12-28T13:45:44.122886Z",
     "shell.execute_reply.started": "2024-12-28T13:45:30.467693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "print(huggingface_hub.__version__)\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict\n",
    "import sys\n",
    "import torch\n",
    "from transformers import TrainingArguments, HfArgumentParser, Trainer, AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:45:44.126215Z",
     "iopub.status.busy": "2024-12-28T13:45:44.125430Z",
     "iopub.status.idle": "2024-12-28T13:46:00.745573Z",
     "shell.execute_reply": "2024-12-28T13:46:00.744903Z",
     "shell.execute_reply.started": "2024-12-28T13:45:44.126180Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"12953d2c5e63b221c7e2412992f16bac816d7870\")\n",
    "wandb.init(project=\"qwen-2.5-3b-lora\",\n",
    "           name=\"test_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:46:00.746604Z",
     "iopub.status.busy": "2024-12-28T13:46:00.746390Z",
     "iopub.status.idle": "2024-12-28T13:46:00.752433Z",
     "shell.execute_reply": "2024-12-28T13:46:00.751616Z",
     "shell.execute_reply.started": "2024-12-28T13:46:00.746575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the arguments required for the main program.\n",
    "# NOTE: You can customize any arguments you need to pass in.\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"Arguments for model\n",
    "    \"\"\"\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the LLM to fine-tune or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype.\"\n",
    "            ),\n",
    "            \"choices\": [\"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    # TODO: add your model arguments here\n",
    "    model_name_or_path = \"/kaggle/input/qwen2.5/transformers/3b/1\"  \n",
    "    torch_dtype = \"float32\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"Arguments for data\n",
    "    \"\"\"\n",
    "    dataset_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the fine-tuning dataset or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    # TODO: add your data arguments here\n",
    "    dataset_path = \"/kaggle/input/alpaca-language-instruction-training/train.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T03:00:47.498023Z",
     "iopub.status.busy": "2024-12-28T03:00:47.497831Z",
     "iopub.status.idle": "2024-12-28T03:00:47.559377Z",
     "shell.execute_reply": "2024-12-28T03:00:47.558541Z",
     "shell.execute_reply.started": "2024-12-28T03:00:47.498005Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def LoRA_finetune():\n",
    "    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))  # 解析器\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    #print(training_args)\n",
    "    \n",
    "    model_path = \"/kaggle/input/qwen2.5/transformers/3b/1\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=\"auto\",device_map=\"auto\")\n",
    "    model.to(device)\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        task_type = TaskType.CAUSAL_LM,\n",
    "        target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        inference_mode = False,\n",
    "        r = 8,\n",
    "        lora_alpha = 32,\n",
    "        lora_dropout = 0.1\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    dataset = datasets.load_dataset('csv', data_files=data_args.dataset_path)\n",
    "\n",
    "\n",
    "    def data_collator(batch: List[Dict]):\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        max_length = 0\n",
    "\n",
    "        for sample in batch:\n",
    "            \n",
    "            instruction_text = sample.get(\"instruction\",\"\")\n",
    "            input_text = sample.get(\"input\",\"\")\n",
    "            output_text = sample.get(\"output\",\"\")\n",
    "\n",
    "            SYSTEM_PROMPT = '''You are an intelligent assistant capable of answering various types of questions based on context. \n",
    "                            Carefully read each instruction and use logical reasoning to determine the best answer. \n",
    "                            Provide your answer after considering relevant input.'''\n",
    "\n",
    "            question = f\"{SYSTEM_PROMPT}{instruction_text}{input_text}\"\n",
    "            \n",
    "\n",
    "            if not output_text.strip():\n",
    "                output_text = \"<empty>\"\n",
    "\n",
    "\n",
    "            input_ids = tokenizer.encode_plus(\n",
    "                question, \n",
    "                return_tensors = \"pt\", \n",
    "                max_length = tokenizer.model_max_length, \n",
    "                truncation = True, \n",
    "                padding = False, \n",
    "            ).input_ids\n",
    "\n",
    "            # 构建输出序列\n",
    "            output_ids = tokenizer.encode_plus(\n",
    "                output_text,\n",
    "                return_tensors = \"pt\",\n",
    "                max_length = tokenizer.model_max_length,\n",
    "                truncation = True,\n",
    "                padding = False,\n",
    "            ).input_ids\n",
    "            \n",
    "            #input_ids = (instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id])\n",
    "            full_input = torch.cat([input_ids,output_ids],dim=1)\n",
    "            inputs.append(full_input)\n",
    "            \n",
    "            #attention_mask = (instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1])\n",
    "            #labels = ([-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]) #只计算output的loss\n",
    "            labels_tensor = torch.full_like(full_input, -100) # 用-100填充表示这些位置在损失计算中被忽略\n",
    "            labels_tensor[:, input_ids.shape[1]:] = full_input[:, input_ids.shape[1]:]\n",
    "            labels.append(labels_tensor)\n",
    "            \n",
    "            if full_input.shape[1] > max_length:\n",
    "                max_length = full_input.shape[1]\n",
    "\n",
    "        inputs = [torch.nn.functional.pad(single_input, (0,max_length - single_input.size(1)),value=tokenizer.pad_token_id) for single_input in inputs]\n",
    "        labels = [torch.nn.functional.pad(single_label, (0, max_length - single_label.size(1)), value=-100) for single_label in labels]\n",
    "\n",
    "        inputs = torch.cat(inputs, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": (inputs != tokenizer.pad_token_id).to(dtype=torch.int)\n",
    "        }\n",
    "\n",
    "  \n",
    "    trainer = Trainer(\n",
    "        args = training_args,\n",
    "        model = model, \n",
    "        tokenizer = tokenizer,\n",
    "        data_collator = data_collator,\n",
    "        train_dataset = dataset[\"train\"],\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def LoRA_finetune():\n",
    "    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))  # 解析器\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    \n",
    "    model_path = \"/kaggle/input/qwen2.5/transformers/3b/1\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=\"auto\")\n",
    "    model.to(device)\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        task_type = TaskType.CAUSAL_LM,\n",
    "        target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "        inference_mode = False,\n",
    "        r = 8,\n",
    "        lora_alpha = 32,\n",
    "        lora_dropout = 0.1\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    dataset = datasets.load_dataset('csv', data_files=data_args.dataset_path)\n",
    "\n",
    "    def data_collator(batch: List[Dict]):\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        mask = []\n",
    "        max_length = 0\n",
    "        SYSTEM_PROMPT = '''You are an intelligent assistant capable of answering various types of questions based on context. \n",
    "                            Carefully read each instruction and use logical reasoning to determine the best answer. \n",
    "                            Provide your answer after considering relevant input.'''\n",
    "\n",
    "        for sample in batch:\n",
    "            \n",
    "            instruction_text = sample.get(\"instruction\",\"\")\n",
    "            input_text = sample.get(\"input\",\"\")\n",
    "            output_text = sample.get(\"output\",\"\")\n",
    "\n",
    "            question = f\"instruction:{instruction_text}input:{input_text}\"\n",
    "            # 因为prompt里面提到了instruction和input所以这里显式的给出\n",
    "            # 鉴于目标是问答机器人，我思考是不是可以更换更符合语境的prompt\n",
    "\n",
    "            messages = [\n",
    "                {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "                {\"role\":\"user\",\"content\":question}\n",
    "            ]\n",
    "\n",
    "            prompt = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "\n",
    "            model_inputs = tokenizer(\n",
    "                [prompt], \n",
    "                return_tensors=\"pt\", \n",
    "                max_length = tokenizer.model_max_length,\n",
    "                truncation = True,\n",
    "                padding = False,).input_ids  #[1,71]\n",
    "            model_outputs = tokenizer(\n",
    "                [output_text], \n",
    "                return_tensors=\"pt\", \n",
    "                max_length = tokenizer.model_max_length,\n",
    "                truncation = True,\n",
    "                padding = False,).input_ids\n",
    "\n",
    "            mask_inputs = tokenizer(\n",
    "                [prompt], \n",
    "                return_tensors=\"pt\", \n",
    "                max_length = tokenizer.model_max_length,\n",
    "                truncation = True,\n",
    "                padding = False,).attention_mask\n",
    "            mask_outputs = tokenizer(\n",
    "                [output_text], \n",
    "                return_tensors=\"pt\", \n",
    "                max_length = tokenizer.model_max_length,\n",
    "                truncation = True,\n",
    "                padding = False,).attention_mask\n",
    "\n",
    "            token = torch.tensor(tokenizer.pad_token_id).unsqueeze(0)\n",
    "            token = token.unsqueeze(0)\n",
    "            token_one = torch.tensor(1).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            full_inputs = torch.cat((model_inputs,model_outputs,token),dim=1)\n",
    "            attention_mask = torch.cat((mask_inputs,mask_outputs,token_one),dim=1)\n",
    "            label_length = model_inputs.shape[1] + model_outputs.shape[1] + 1 #201\n",
    "            labels_tensor = torch.full((1,label_length), -100) #[201]\n",
    "            labels_tensor[:,model_inputs.shape[1]:-1] = model_outputs\n",
    "            labels_tensor[:,-1] = token\n",
    "            # print(type(full_inputs),type(full_labels)) #list\n",
    "            \n",
    "            inputs.append(full_inputs)\n",
    "            labels.append(labels_tensor)\n",
    "            mask.append(attention_mask)\n",
    "            # print(type(inputs),type(labels)) #list\n",
    "            \n",
    "            if model_outputs.shape[1] > max_length:\n",
    "                max_length = model_outputs.shape[1]\n",
    "                \n",
    "        # 保持不变\n",
    "        inputs = [torch.nn.functional.pad(single_input, (0,max_length - single_input.size(1)),value=tokenizer.pad_token_id) for single_input in inputs]\n",
    "        labels = [torch.nn.functional.pad(single_label, (0, max_length - single_label.size(1)), value=-100) for single_label in labels]\n",
    "        mask = [torch.nn.functional.pad(single_mask, (0, max_length - single_mask.size(1)), value=0) for single_mask in mask]\n",
    "\n",
    "        inputs = torch.cat(inputs, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "        mask = torch.cat(mask, dim=0)\n",
    "        # print(type(mask))\n",
    "    \n",
    "            \n",
    "        return {\n",
    "            \"input_ids\": inputs,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": mask\n",
    "        }\n",
    "\n",
    "  \n",
    "    trainer = Trainer(\n",
    "        args = training_args,\n",
    "        model = model, \n",
    "        tokenizer = tokenizer,\n",
    "        data_collator = data_collator,\n",
    "        train_dataset = dataset[\"train\"],\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-28T13:47:13.675176Z",
     "iopub.status.busy": "2024-12-28T13:47:13.674724Z",
     "iopub.status.idle": "2024-12-28T13:48:11.008079Z",
     "shell.execute_reply": "2024-12-28T13:48:11.006943Z",
     "shell.execute_reply.started": "2024-12-28T13:47:13.675138Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    \"notebook\", \n",
    "    \"--output_dir\", \"/kaggle/working/qwen2.5/3b-lora\",\n",
    "    \"--run_name\",\"test_2\",\n",
    "    \"--learning_rate\",\"1e-5\",\n",
    "    \"--num_train_epochs\", \"5\",  # 通常3-5个epoch即可收敛，长时间训练可能会过拟合\n",
    "    \"--per_device_train_batch_size\", \"1\",  # 每个GPU上的大小\n",
    "    \"--overwrite_output_dir\",\"True\",  #开发过程中覆盖旧的文件\n",
    "    \"--save_steps\", \"5000\",\n",
    "    \"--save_total_limit\", \"10\",\n",
    "    \"--logging_steps\",\"50\",\n",
    "    \"--logging_dir\", \"/kaggle/working/\",\n",
    "    \"--remove_unused_columns\",\"False\",\n",
    "    \"--dataloader_drop_last\", \"True\",\n",
    "    '--seed','42',\n",
    "    \"--fp16\",\"True\",  # 开启混合精度加速\n",
    "]\n",
    "LoRA_finetune()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-28T03:02:15.317293Z",
     "iopub.status.idle": "2024-12-28T03:02:15.317656Z",
     "shell.execute_reply": "2024-12-28T03:02:15.317505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#合并基础模型和lora模型\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/3b-instruct/1\"\n",
    "lora_dir = \"/kaggle/input/lora-qwen2-5-1-5b-batchsize1/qwen2.5/1.5b-lora/checkpoint-103520\" #换成自己跑出来的lora\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(model, lora_dir).to(device)\n",
    "#print(model)\n",
    "# 合并model, 同时保存 token\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(\"lora_output_prompt8\")\n",
    "tokenizer.save_pretrained(\"lora_output_prompt8\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4061777,
     "sourceId": 7056498,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6386098,
     "sourceId": 10315389,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164048,
     "modelInstanceId": 141456,
     "sourceId": 166243,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164048,
     "modelInstanceId": 141458,
     "sourceId": 166245,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164048,
     "modelInstanceId": 141462,
     "sourceId": 166249,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
