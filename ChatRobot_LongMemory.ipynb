{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-16T02:35:42.340157Z",
     "iopub.status.busy": "2025-01-16T02:35:42.339838Z",
     "iopub.status.idle": "2025-01-16T02:36:06.987115Z",
     "shell.execute_reply": "2025-01-16T02:36:06.986280Z",
     "shell.execute_reply.started": "2025-01-16T02:35:42.340133Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.4/326.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.4/130.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25 -q\n",
    "!pip install faiss-cpu -q\n",
    "!pip install sentence-transformers -q\n",
    "!pip install langchain -q\n",
    "!pip install peft==0.12.0 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档分块与向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T02:36:13.439637Z",
     "iopub.status.busy": "2025-01-16T02:36:13.439346Z",
     "iopub.status.idle": "2025-01-16T02:36:13.459821Z",
     "shell.execute_reply": "2025-01-16T02:36:13.459133Z",
     "shell.execute_reply.started": "2025-01-16T02:36:13.439611Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共加载了2个文档\n"
     ]
    }
   ],
   "source": [
    "# 从.txt文件加载文档\n",
    "import os\n",
    "\n",
    "def load_txt_documents(folder_path):\n",
    "    documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path,'r',encoding='utf-8') as file:\n",
    "                documents.append(file.read())  # 只保留文档内容\n",
    "    return documents\n",
    "\n",
    "#测试该函数\n",
    "files = \"/kaggle/input/knowledge-documents\"\n",
    "documents = load_txt_documents(files)  # 大小为 num_of_file * 一大块文档字符串\n",
    "print(f\"共加载了{len(documents)}个文档\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T02:36:17.219611Z",
     "iopub.status.busy": "2025-01-16T02:36:17.219330Z",
     "iopub.status.idle": "2025-01-16T02:36:18.174835Z",
     "shell.execute_reply": "2025-01-16T02:36:18.174047Z",
     "shell.execute_reply.started": "2025-01-16T02:36:17.219589Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n"
     ]
    }
   ],
   "source": [
    "# 文档分块与预处理\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter  \n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "import re\n",
    "    \n",
    "def split_documents_by_paragraph(documents: List[str], chunk_size: int = 500) -> List[str]:  \n",
    "    \"\"\"  \n",
    "    将文档列表按段完整分块，每段按照 `\\n\\n` 优先切割，  \n",
    "    遇到单独的段落超出 chunk_size 时将其划归下一个 chunk。  \n",
    "    \"\"\"  \n",
    "    chunks = []  \n",
    "\n",
    "    for document in documents:  \n",
    "        # 遍历每一个文档\n",
    "        document_cleaned = re.sub(r'[ \\t]+', ' ', document.strip()) # 用单个空格代替空格和制表符\n",
    "        paragraphs = document_cleaned.split(\"\\n\\n\")  # 按照空行进行段落划分\n",
    "\n",
    "        # 暂时没有处理一个段落一个chunk装不下的问题（考虑如果装不下需要有合理的overlap部分）\n",
    "        for paragraph in paragraphs:  \n",
    "            if paragraph.strip():  \n",
    "                chunks.append(paragraph.strip())  \n",
    "\n",
    "    return chunks \n",
    "   \n",
    "# 测试该函数\n",
    "chunks = split_documents_by_paragraph(documents, chunk_size=500)\n",
    "print(len(chunks))\n",
    "# for idx,chunk in enumerate(chunks):\n",
    "#     print(f\"分块{idx+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用BM2,包含向量化+检索结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T02:36:23.674038Z",
     "iopub.status.busy": "2025-01-16T02:36:23.673522Z",
     "iopub.status.idle": "2025-01-16T02:36:38.265779Z",
     "shell.execute_reply": "2025-01-16T02:36:38.265074Z",
     "shell.execute_reply.started": "2025-01-16T02:36:23.674009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 根据bm25检索文本内部自动计算\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import jieba\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer \n",
    "\n",
    "stopwords = set([\"的\",\"是\",\"了\",\"请\",\"简述\",\"论证\",\"回答\",\"解释\",\"说明\",\"一下\",\"什么\"])\n",
    "\n",
    "def create_bm25(chunks, stopwords=set([\"的\",\"是\",\"了\",\"请\",\"简述\",\"论证\",\"回答\",\"解释\",\"说明\",\"一下\",\"什么\"])\n",
    "):\n",
    "    \"\"\"  \n",
    "    为文本块创建 BM25 索引和嵌入向量。  \n",
    "    chunks: 文本块列表。  \n",
    "    return: BM25 索引。  \n",
    "    \"\"\" \n",
    "    # 分词函数\n",
    "    def preprocess_text(text):\n",
    "        # 使用 jieba 分词并移除停用词\n",
    "        return [word for word in jieba.cut(text) if word not in stopwords]\n",
    "\n",
    "    tokenized_chunks = [preprocess_text(chunk) for chunk in chunks]  \n",
    "    bm25 = BM25Okapi(tokenized_chunks)  # k1=1.5, b=0.75  # 接受的文档（集合）是分词后的词列表[[],[],[]]\n",
    "    \n",
    "    # 对文本向量化储存并且一起返回\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = embedding_model.encode(chunks, convert_to_numpy=True)\n",
    "\n",
    "    return bm25, embedding_model, embeddings\n",
    "\n",
    "def bm25_search(query, bm25, stopwords=set([\"的\",\"是\",\"了\",\"请\",\"简述\",\"论证\",\"回答\",\"解释\",\"说明\",\"一下\",\"什么\"])):\n",
    "    \"\"\"  \n",
    "    使用 BM25 索引查找与用户查询相关的 top_k 文本块。  \n",
    "    :param query: 用户查询，字符串。  \n",
    "    :param bm25: 已创建的 BM25 索引。  \n",
    "    :param chunks: 文本块集合。  \n",
    "    :param top_k: 返回前 K 个相关文本块。  \n",
    "    :return: 检索到的文本块及其分数。  \n",
    "    \"\"\"\n",
    "    query_tokens = [word for word in jieba.cut(query) if word not in stopwords]\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    # ranked_indices = sorted(range(len(scores)), key=lambda i:scores[i],reverse=True)[:top_k]\n",
    "    # return [(chunks[i],scores[i]) for i in ranked_indices]\n",
    "    return scores\n",
    "\n",
    "def similarity_search(query, embedding_model, embeddings):  \n",
    "    \"\"\"  \n",
    "    使用嵌入模型基于语义相似性查找相关文本块。  \n",
    "    :param query: 用户查询，字符串。  \n",
    "    :param embedding_model: 已加载的嵌入模型。  \n",
    "    :param embeddings: 文本块嵌入向量数组。  \n",
    "    :param chunks: 文本块集合。  \n",
    "    :param top_k: 返回前 K 个相关文本块。  \n",
    "    :return: 检索到的文本块及其相似性分数。  \n",
    "    \"\"\"  \n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        return dot_product / (norm_vec1 * norm_vec2)\n",
    "    query_embedding = embedding_model.encode(query, convert_to_numpy=True)  \n",
    "    # 计算余弦相似度  \n",
    "    similarity_scores = np.array([cosine_similarity(prompt, query_embedding) for prompt in embeddings]) \n",
    "    # ranked_indices = np.argsort(similarity_scores)[::-1][:top_k]  \n",
    "    # return [(chunks[i], similarity_scores[i]) for i in ranked_indices]\n",
    "    return similarity_scores\n",
    "\n",
    "def rank_fusion(bm25_score, similarity_score, bm25_weight=0.5, embedding_weight=0.5, top_k=5):  \n",
    "    \"\"\"  \n",
    "    使用排名融合结合 BM25 和语义嵌入的结果。  \n",
    "    :param bm25_score: BM25 检索结果 [score]。  \n",
    "    :param similarity_score: 嵌入检索结果 [score]。  \n",
    "    :param bm25_weight: BM25 分数的权重。  \n",
    "    :param embedding_weight: 嵌入分数的权重。  \n",
    "    :param top_k: 返回前 K 个候选文本块。  \n",
    "    :return: 融合后的文本块及评分。  \n",
    "    \"\"\"  \n",
    "    # 加权求和 BM25 分数和相似度分数  \n",
    "    scores = bm25_weight*bm25_score + embedding_weight*similarity_score\n",
    "    # ranked_indices = np.argsort(scores)[::-1][:top_k]  \n",
    "\n",
    "    # 按照分数排序并返回 top_k 结果 \n",
    "    # ranked_chunks = [(chunks[i], scores[i]) for i in ranked_indices]\n",
    "    # return ranked_chunks \n",
    "    return scores\n",
    "\n",
    "# 测试\n",
    "# user_input = \"新民主主义革命的三大法宝\"\n",
    "# user_input = \"你好\" \n",
    "# user_input = \"领导人的精髓\"\n",
    "# user_input = \"一国两制的基本内容\"\n",
    "# bm25, embedding_model, embeddings = create_bm25(chunks)   # 实际使用时作为chat函数输入\n",
    "# bm25_scores = bm25_search(user_input, bm25)\n",
    "# embedding_scores = similarity_search(user_input, embedding_model, embeddings)\n",
    "# fused_scores = rank_fusion(bm25_scores, embedding_scores)\n",
    "# top_k = 5\n",
    "# ranked_indices = np.argsort(fused_scores)[::-1][:top_k]  \n",
    "# retrieve_chunks = [(chunks[i], fused_scores[i]) for i in ranked_indices]\n",
    "\n",
    "# #增加一个:过滤分数小于1的结果\n",
    "# filtered_chunks = [sub_chunk for sub_chunk in retrieve_chunks if sub_chunk[1] > 1]  \n",
    "\n",
    "# for sub_chunk in filtered_chunks:  \n",
    "#     print(f\"Sub-chunk : \\n{sub_chunk[0]}\\n(Score: {sub_chunk[1]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检索函数包装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T02:36:47.642400Z",
     "iopub.status.busy": "2025-01-16T02:36:47.641784Z",
     "iopub.status.idle": "2025-01-16T02:36:47.647701Z",
     "shell.execute_reply": "2025-01-16T02:36:47.646957Z",
     "shell.execute_reply.started": "2025-01-16T02:36:47.642374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def retrieve_text_by_bm25_and_similarity(query_message, bm25, embedding_model, embeddings, chunks,top_k = 5):\n",
    "    '''\n",
    "    通过BM25和语义相似度检索文本\n",
    "    :param query_message: 用户查询字符串。\n",
    "    :param bm25: 已创建的BM25索引。\n",
    "    :param embedding_model: 已加载的嵌入模型。\n",
    "    :param embeddings: 文本块嵌入向量数组。\n",
    "    :param chunks: 文本块集合。\n",
    "    :param top_k: 返回前K个相关文本块。\n",
    "    :return: 检索到的文本块。\n",
    "    '''\n",
    "    bm25_scores = bm25_search(query_message, bm25)\n",
    "    embedding_scores = similarity_search(query_message, embedding_model, embeddings)\n",
    "    fused_scores = rank_fusion(bm25_scores, embedding_scores)\n",
    "    ranked_indices = np.argsort(fused_scores)[::-1][:top_k]  \n",
    "    \n",
    "    retrieve_texts = []\n",
    "    for i in ranked_indices:\n",
    "        retrieve_texts.append((chunks[i], fused_scores[i]))\n",
    "    #print(fused_scores)\n",
    "    filtered_chunks = [(text, score) for text, score in retrieve_texts if score > 1]  \n",
    "    return filtered_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 和我们的聊天机器人进行整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T02:36:50.265082Z",
     "iopub.status.busy": "2025-01-16T02:36:50.264770Z",
     "iopub.status.idle": "2025-01-16T02:37:30.410504Z",
     "shell.execute_reply": "2025-01-16T02:37:30.409583Z",
     "shell.execute_reply.started": "2025-01-16T02:36:50.265058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02176e0169414f068a91f5d4f09f434c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/3b/1\"\n",
    "lora_dir = \"/kaggle/input/lora-3b/transformers/default/1\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = PeftModel.from_pretrained(model, lora_dir)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T02:37:39.512124Z",
     "iopub.status.busy": "2025-01-16T02:37:39.511842Z",
     "iopub.status.idle": "2025-01-16T02:37:43.595462Z",
     "shell.execute_reply": "2025-01-16T02:37:43.594626Z",
     "shell.execute_reply.started": "2025-01-16T02:37:39.512102Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.699 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045d6c997e6740c1bc130aa9c3cd78fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3ccca5d6314808a931286a8d81b771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745497a31d3f4bb1aeeeeab45b2bebb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2357c41c955d49f088b3d95ce5a27b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136784edbb0b4e2e867759addf27a60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938699b61f1740468cbad60b64529f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbc2cf07e25442e8d2af5691e3b3bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c3ddbdd71a425eae4dc46605cd4bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e520f830e69e458f8d367253567461b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c2444d0c024c77b2c3eb1c568a8c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696e2496e0514ffdbb88a1b1b01a4bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c90c22f1c14ca689084088cec92931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 准备知识库\n",
    "\n",
    "files = \"/kaggle/input/knowledge-documents\"\n",
    "documents = load_txt_documents(files)\n",
    "chunks = split_documents_by_paragraph(documents, chunk_size=500)\n",
    "bm25, embedding_model, embeddings = create_bm25(chunks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T02:38:11.864700Z",
     "iopub.status.busy": "2025-01-16T02:38:11.864357Z",
     "iopub.status.idle": "2025-01-16T02:38:11.871411Z",
     "shell.execute_reply": "2025-01-16T02:38:11.870514Z",
     "shell.execute_reply.started": "2025-01-16T02:38:11.864652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ChatHistoryTxT:\n",
    "    def __init__(self, filepath=\"/kaggle/working/chat_history.txt\"):\n",
    "        self.filepath = filepath\n",
    "        if not os.path.exists(self.filepath):\n",
    "            with open(self.filepath,\"w\",encoding=\"utf-8\") as f:\n",
    "                pass\n",
    "\n",
    "    def add_chat_history(self, user_input, user_prompt, assistant_response):\n",
    "        with open(self.filepath, 'a', encoding='utf-8') as f:\n",
    "            f.write(f\"用户：{user_input}\\n\")\n",
    "            f.write(f\"用户：{user_prompt}\\n\")\n",
    "            f.write(f\"助手：{assistant_response}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    def search_chat_history(self, query, chat_top_k=1):\n",
    "        chat_doc = []\n",
    "        with open(self.filepath,'r',encoding='utf-8') as file:\n",
    "            chat_doc.append(file.read())\n",
    "        chat_chunks = split_documents_by_paragraph(chat_doc, chunk_size=500)\n",
    "        #print(len(chat_chunks))\n",
    "        chat_bm25, chat_embedding_model, chat_embeddings = create_bm25(chat_chunks)\n",
    "        chat_context = retrieve_text_by_bm25_and_similarity(query, chat_bm25, chat_embedding_model, chat_embeddings, chat_chunks,chat_top_k)  # 会将得分最高的k个文档合并返回,k=1效果会更好\n",
    "        chat_context = '\\n'.join(text.replace('\\n', ', ') for text, score in chat_context) # 如果一段上下文里有换行,则将其用逗号拼接为一个长句\n",
    "        #print(chat_context)\n",
    "        if not chat_context.strip():\n",
    "            chat_context = \"无\"\n",
    "        return chat_context\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T02:42:51.899645Z",
     "iopub.status.busy": "2025-01-16T02:42:51.899351Z",
     "iopub.status.idle": "2025-01-16T02:42:51.910320Z",
     "shell.execute_reply": "2025-01-16T02:42:51.909390Z",
     "shell.execute_reply.started": "2025-01-16T02:42:51.899623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "max_input_length = model.config.max_position_embeddings  #32768\n",
    "max_round = 5\n",
    "long_memory = True\n",
    "my_chat_history = ChatHistoryTxT(filepath=\"/kaggle/working/chat_history.txt\")\n",
    "\n",
    "def chat_with_bm25(bm25, chunks, max_input_length, model, tokenizer, device):  \n",
    "    \n",
    "    print(\"开始聊天（输入 '\\\\quit' 结束对话，输入 '\\\\newsession' 开启新会话）：\")  \n",
    "    #system_prompt = \"你是一名助手，请根据用户问题以及提供的相关文档，提炼出你的答案，并对用户进行回答，如果没有相关文档，就请你自己对用户问题进行回答。\"\n",
    "    #system_prompt = \"你是一名老师，性格温柔耐心，语气充满关怀，擅长引导学生学习知识，请根据用户问题以及提供的相关文档，提炼出你的答案，并对用户进行回答，如果没有相关文档，就请你自己对用户问题进行回答。\"\n",
    "    #system_prompt = \"你是一名老师，性格温柔耐心，语气充满关怀，擅长引导学生学习知识，你的目标是：如果用户有疑问，耐心解答，并根据提供的相关文档来提供准确的回答，如果用户的说法错误，你会指出用户的错误，如果用户的说法正确，你会给予表扬和鼓励。\"\n",
    "    system_prompt = '''\n",
    "                    你是一名老师，性格温柔耐心，语气充满关怀，擅长引导学生学习知识。用户是你的学生，会与你展开交流。\n",
    "                    根据学生的发言，你需要遵循以下规则：  \n",
    "                    1、如果学生提出疑问，耐心解答。  \n",
    "                    2、如果学生回答有误，温柔地引导纠正，解释正确答案，并提供补充知识点。  \n",
    "                    3、如果学生回答正确，热情表扬并适当鼓励。  \n",
    "                    4、依据相关文档回答问题；如果没有相关文档，请自行解答问题。\n",
    "                    \n",
    "                    请严格遵循以下格式回答每个问题：  \n",
    "                    （动作）语言【附加信息】  \n",
    "                    动作：用括号“（）”标注动作或表情，比如（敲黑板），（摸头），（鼓掌）。  \n",
    "                    语言：你的主要回答内容，不需要特殊标记。  \n",
    "                    附加信息：用中括号“【】”补充对回答的情绪或状态描述，比如【鼓励】【严肃】【亲切】。 \n",
    "                    \n",
    "                    以下是示例对话：  \n",
    "                    用户：我认为学习政治没有意义。  \n",
    "                    助手：（挥舞教鞭）你的说法不对，学习政治有助于全面认识社会【严肃】  \n",
    "                    \n",
    "                    用户：毛泽东思想的核心是“实事求是”吗？  \n",
    "                    助手：（点头微笑）正确，毛泽东思想的核心正是“实事求是”【鼓励】  \n",
    "                    '''\n",
    "    system_prompt = re.sub(r\"[ \\t]+\",\"\",system_prompt)\n",
    "    chat_history = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\":\"user\", \"content\": \"你必须严格按照以下格式回答问题：（动作）语言【附加信息】\"}\n",
    "                ]\n",
    "    while True:  \n",
    "        user_input = input(\"用户：\")  \n",
    "        if user_input.lower() == \"\\\\quit\":  \n",
    "            print(\"结束对话。\")  \n",
    "            break  \n",
    "\n",
    "        elif user_input.lower() == \"\\\\newsession\":  \n",
    "            print(\"开启新会话。\")  \n",
    "            chat_history = [{\"role\": \"system\", \"content\": system_prompt}]  \n",
    "            i = 0\n",
    "            continue \n",
    "\n",
    "        # 检索相关文档\n",
    "        context = retrieve_text_by_bm25_and_similarity(user_input, bm25, embedding_model, embeddings, chunks,top_k = 1)  # 会将得分最高的k个文档合并返回,k=1效果会更好\n",
    "        context = '\\n'.join(text.replace('\\n', ', ') for text, score in context) # 如果一段上下文里有换行,则将其用逗号拼接为一个长句\n",
    "        if not context.strip():  # 使用 strip() 检查空白  \n",
    "            context = \"无\"\n",
    "        #print(\"文档检索：\",context)\n",
    "        if long_memory and os.stat(\"/kaggle/working/chat_history.txt\").st_size > 0:\n",
    "            # 检索相关对话历史\n",
    "            history_context = my_chat_history.search_chat_history(query=user_input, chat_top_k=1)\n",
    "            #print(\"对话历史检索：\",history_context)\n",
    "            prompt = f\"下面是与用户问题相关的文档内容:{context}\\n下面是与用户问题相关的对话历史：{history_context}\\n请根据这些文档内容回答以下用户问题: {user_input}\"\n",
    "        else:\n",
    "            prompt = f\"下面是与用户问题相关的文档内容:{context}\\n请根据这些文档内容回答以下问题:用户问题: {user_input}\"\n",
    "        print(\"prompt:\",prompt)\n",
    "        chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        chat_history.append({\"role\": \"user\", \"content\": \"你必须严格按照以下格式回答问题：（动作）语言【附加信息】\"})\n",
    "        text = tokenizer.apply_chat_template(chat_history, tokenize=False, add_generation_prompt=True)\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        #while len(model_inputs[\"input_ids\"][0]) > max_input_length: \n",
    "        while len(chat_history) > max_round:\n",
    "            #弹出最开始的一轮对话\n",
    "            history1 = chat_history.pop(2)\n",
    "            history2 = chat_history.pop(2)\n",
    "            history3 = chat_history.pop(2)\n",
    "            # print(\"1\",history1[\"content\"])\n",
    "            # print(\"2\",history2[\"content\"])\n",
    "            # print(\"3\",history3[\"content\"])\n",
    "            #将移除的对话历史移动到聊天历史知识库\n",
    "            my_chat_history.add_chat_history(history1[\"content\"],history2[\"content\"],history3[\"content\"])\n",
    "            text = tokenizer.apply_chat_template(chat_history, tokenize=False, add_generation_prompt=True)\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "        #print(text)\n",
    "        generated_ids = model.generate(input_ids=model_inputs[\"input_ids\"],\n",
    "                                       attention_mask=model_inputs[\"attention_mask\"],\n",
    "                                       pad_token_id=tokenizer.pad_token_id,\n",
    "                                       max_new_tokens=512,\n",
    "                                       do_sample=True, \n",
    "                                       temperature=0.7,\n",
    "                                       top_p=0.9\n",
    "                                      )  \n",
    "\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        print(f\"助手： {response}\")  \n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T02:42:54.460134Z",
     "iopub.status.busy": "2025-01-16T02:42:54.459807Z",
     "iopub.status.idle": "2025-01-16T02:48:22.519781Z",
     "shell.execute_reply": "2025-01-16T02:48:22.518522Z",
     "shell.execute_reply.started": "2025-01-16T02:42:54.460105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化检索数据库...\n",
      "开始聊天（输入 '\\quit' 结束对话，输入 '\\newsession' 开启新会话）：\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户： 你好\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f0d821a1154fcebb52bc87e07cff04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: 下面是与用户问题相关的文档内容:无\n",
      "请根据这些文档内容回答以下问题:用户问题: 你好\n",
      "助手： (微笑)你好，很高兴见到你。有什么可以帮助你的吗？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户： 新民主主义革命的三大法宝是什么\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0c7303f6bb48d9a7c08a1fa2a67d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: 下面是与用户问题相关的文档内容:新民主主义革命的三大法宝：统一战线、武装斗争、党的建设。\n",
      "请根据这些文档内容回答以下问题:用户问题: 新民主主义革命的三大法宝是什么\n",
      "助手： （敲黑板）新民主主义革命的三大法宝是：统一战线、武装斗争、党的建设。【严肃】\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户： 邓小平思想的精髓是什么\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61742b85f774fb1850dd693c56b6fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1831a844394f96a04c7bef82df206c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c427faa717644554b81faced691df8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: 下面是与用户问题相关的文档内容:二、邓小平理论的精髓, 历代领导人的精髓, 毛泽东：实事求是, 邓小平：解放思想，实事求是, 江泽民：解放思想，实事求是，与时俱进, 胡锦涛：解放思想，实事求是，与时俱进，求真务实\n",
      "下面是与用户问题相关的对话历史：无\n",
      "请根据这些文档内容回答以下用户问题: 邓小平思想的精髓是什么\n",
      "助手： （竖起大拇指）邓小平思想的精髓就是“解放思想，实事求是”【鼓励】\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户： 新民主主义革命的三大法宝是什么\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e981f8d7fbf7491c9220b89f59993a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246b59b6540447a9bb467d9fdc5433cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8d28e754414ba288b47e6c17617f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: 下面是与用户问题相关的文档内容:新民主主义革命的三大法宝：统一战线、武装斗争、党的建设。\n",
      "下面是与用户问题相关的对话历史：无\n",
      "请根据这些文档内容回答以下用户问题: 新民主主义革命的三大法宝是什么\n",
      "助手： 【严肃】新民主主义革命的三大法宝是：统一战线、武装斗争、党的建设。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户： 新民主主义革命的三大法宝\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd2b20986c5481190828a92884fce7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2a2eb686874723973a7a82cea2108e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae17f047ca9c4699a382f239122d3a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: 下面是与用户问题相关的文档内容:新民主主义革命的三大法宝：统一战线、武装斗争、党的建设。\n",
      "下面是与用户问题相关的对话历史：用户：下面是与用户问题相关的文档内容:新民主主义革命的三大法宝：统一战线、武装斗争、党的建设。, 请根据这些文档内容回答以下问题:用户问题: 新民主主义革命的三大法宝是什么, 用户：你必须严格按照以下格式回答问题：（动作）语言【附加信息】, 助手：（敲黑板）新民主主义革命的三大法宝是：统一战线、武装斗争、党的建设。【严肃】\n",
      "请根据这些文档内容回答以下用户问题: 新民主主义革命的三大法宝\n",
      "助手： 根据用户问题: 新民主主义革命的三大法宝是什么, 用户：你必须严格按照以下格式回答问题：（动作）语言【附加信息】, 助手：（敲黑板）新民主主义革命的三大法宝是：统一战线、武装斗争、党的建设。【严肃】\n",
      "根据文档内容：新民主主义革命的三大法宝是：统一战线、武装斗争、党的建设。\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-cd52a8a27258>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"初始化检索数据库...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mchat_with_bm25\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbm25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_input_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#新民主主义革命的三大法宝是什么\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#邓小平思想的精髓是什么\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-62e873002810>\u001b[0m in \u001b[0;36mchat_with_bm25\u001b[0;34m(bm25, chunks, max_input_length, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 ]\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"用户：\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\\\\quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"结束对话。\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# run\n",
    "print(\"初始化检索数据库...\")\n",
    "chat_with_bm25(bm25, chunks, max_input_length, model, tokenizer, device)\n",
    "#新民主主义革命的三大法宝是什么\n",
    "#邓小平思想的精髓是什么\n",
    "#一国两制的基本内容是什么"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6403355,
     "sourceId": 10345212,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141462,
     "sourceId": 166249,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 203876,
     "modelInstanceId": 181643,
     "sourceId": 213094,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
